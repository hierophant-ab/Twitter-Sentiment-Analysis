{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0681096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow import keras\n",
    "from keras import layers,regularizers\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score,roc_curve,auc,roc_auc_score\n",
    "import emoji\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f34239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"twitter_training.csv\",header=None)\n",
    "df_train=df_train.dropna()\n",
    "df_train[2]=df_train[2].map({\"Positive\":1,\"Negative\":0,\"Neutral\":2,\"Irrelevant\":3})\n",
    "df_train=df_train.rename(columns={3:\"text\",2:\"sentiment\",1:\"category\",0:\"id\"})\n",
    "\n",
    "df_test=pd.read_csv(\"twitter_validation.csv\",header=None)\n",
    "df_test=df_test.dropna()\n",
    "df_test[2]=df_test[2].map({\"Positive\":1,\"Negative\":0,\"Neutral\":2,\"Irrelevant\":3})\n",
    "df_test=df_test.rename(columns={3:\"text\",2:\"sentiment\",1:\"category\",0:\"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURL(text):\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "def removeMentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def hashtagHandling(text):\n",
    "    return re.sub(r'#', '', text)\n",
    "\n",
    "def decodeEmoji(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(text):    \n",
    "    text = text.lower()\n",
    "    text = removeURL(text)\n",
    "    text = removeMentions(text)\n",
    "    text = hashtagHandling(text)\n",
    "    text = decodeEmoji(text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)   # keep only letters & spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column_name):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    try:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    except TypeError:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "    encoded_array = encoder.fit_transform(df[[column_name]])\n",
    "\n",
    "    try:\n",
    "        feature_names = encoder.get_feature_names_out([column_name])\n",
    "    except AttributeError:\n",
    "        feature_names = encoder.get_feature_names([column_name])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df.index)\n",
    "\n",
    "    df= pd.concat([df.drop(columns=[column_name]), encoded_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(df,column_name):\n",
    "    vectorizer=TfidfVectorizer(max_features=5000,stop_words=\"english\",ngram_range=(1,2))\n",
    "    Encoded_Text=vectorizer.fit_transform(df[column_name]).toarray()\n",
    "    return Encoded_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a24a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=one_hot_encode(df_train,\"category\")\n",
    "df_train[\"text\"]=df_train[\"text\"].apply(cleanTweets)\n",
    "\n",
    "df_test=one_hot_encode(df_test,\"category\")\n",
    "df_test[\"text\"]=df_test[\"text\"].apply(cleanTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01923595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(sentences):\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "    model = Word2Vec(tokenized_sentences, \n",
    "                    vector_size=300,  # Dimension of word vectors\n",
    "                    window=5,         # Context window size\n",
    "                    min_count=1,      # Minimum word frequency\n",
    "                    sg=1,             # Skip-gram model\n",
    "                    workers=4)        # Number of threads\n",
    "    return model\n",
    "\n",
    "def text_to_vectors(texts, model):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        # Get vectors for words that exist in the model's vocabulary\n",
    "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "        if word_vectors:\n",
    "            sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.zeros(model.vector_size)\n",
    "        vectors.append(sentence_vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "w2v_model = create_word2vec_model(df_train['text'])\n",
    "\n",
    "encoded_text_train= text_to_vectors(df_train['text'], w2v_model)\n",
    "encoded_text_test= text_to_vectors(df_test['text'], w2v_model)\n",
    "\n",
    "print(\"Training vectors shape:\", encoded_text_train.shape)\n",
    "print(\"Testing vectors shape:\", encoded_text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35099d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(df_train.drop(columns=[\"sentiment\",\"id\",\"text\"]))\n",
    "X_train=np.concatenate((X,encoded_text_train),axis=1)\n",
    "y_train=np.array(df_train[\"sentiment\"])\n",
    "\n",
    "X=np.array(df_test.drop(columns=[\"sentiment\",\"id\",\"text\"]))\n",
    "X_test=np.concatenate((X,encoded_text_test),axis=1)\n",
    "y_test=np.array(df_test[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelrf=RandomForestClassifier(n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "modelrf.fit(X_train,y_train)\n",
    "predictions_train=modelrf.predict(X_train)\n",
    "predictions_test=modelrf.predict(X_test)\n",
    "print(accuracy_score(y_train,predictions_train),\" \",accuracy_score(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(332,)),\n",
    "        layers.Dense(128,activation='relu',),\n",
    "        BatchNormalization(),\n",
    "        layers.Dense(64,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        layers.Dense(32,activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        layers.Dense(4, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=25,verbose=0)\n",
    "# model.evaluate(X_train,y_train,batch_size=64,verbose=2,epochs=1)\n",
    "predictions_train=np.argmax(model.predict(X_train),axis=1)\n",
    "predProb_test=model.predict(X_test)\n",
    "predictions_test=np.argmax(predProb_test,axis=1)\n",
    "print(accuracy_score(y_train,predictions_train),\" \",accuracy_score(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62860744",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train=confusion_matrix(y_train,predictions_train)\n",
    "cr_train=classification_report(y_train,predictions_train)\n",
    "\n",
    "cm_test=confusion_matrix(y_test,predictions_test)\n",
    "cr_test=classification_report(y_test,predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df09279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_auc(y_true, y_pred, classes):\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    aucs = np.array([roc_auc_score(y_true_bin[:, i], y_pred[:, i]) for i in range(len(classes))])\n",
    "    return np.round(aucs,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22867d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_curves(y_true, y_score, classes, title):\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for i, color in zip(range(len(classes)), cycle(['aqua','darkorange','cornflowerblue','green'])):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'Class {classes[i]} (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0,1], [0,1], 'k--', lw=1)\n",
    "    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ace786",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs=np.array(per_class_auc(y_test,modelrf.predict_proba(X_test),classes=[0,1,2,3]))\n",
    "print(f\"Random Forest AUCs: {aucs}\")\n",
    "plot_roc_curves(y_test,modelrf.predict_proba(X_test),classes=[0,1,2,3],title=\"Random Forest ROC- Test\")\n",
    "\n",
    "aucs=np.array(per_class_auc(y_test,predProb_test,classes=[0,1,2,3]))\n",
    "print(f\"Neural Network AUCs: {aucs}\")\n",
    "plot_roc_curves(y_test,predProb_test,classes=[0,1,2,3],title=\"Neural Network ROC- Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98543b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
